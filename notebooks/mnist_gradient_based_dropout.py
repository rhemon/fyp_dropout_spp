# -*- coding: utf-8 -*-
"""MNIST Gradient Based Dropout.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19atLwrwF_afZSqTFIs3m90lVMG1BigZK
"""

import torch
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

from xml.dom.minidom import Attr
import numpy as np
import pandas as pd
import datetime
import torch

from sklearn.model_selection import train_test_split
from sklearn.utils import shuffle 


class BaseLoader:
    """
    Class to laod KEATS dataset as OHV input, although actualy output is essentially indexes of the onehot vector position
    as data will be used in a GritNet model that uses Embedding layer.

    Takes in columns type and output type from cfg
    Columns type to specify which fields to use. Output type Binary or multivariative (support not added yet) 

    """    

    def __init__(self, cfg, checkpoint_folder=None):

        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        self.load_method = cfg.LOAD_METHOD
        try:
            self.class_sizes = cfg.CLASS_SPLIT
        except AttributeError:
            self.class_sizes = None
        
        
        self.checkpoint_folder = checkpoint_folder


    def create_dataset(self):
        pass
    
    def split_class_sets(self, all_class_sets, test_split_ratio=0.2):
        train_sets = []
        test_sets = []
        for class_sets in all_class_sets:
            split = train_test_split(*class_sets, test_size=test_split_ratio, random_state=0)
            train_split = []
            test_split = []
            for i in range(0, len(split), 2):
                train_split.append(split[i])
                test_split.append(split[i+1])
            train_sets.append(train_split)
            test_sets.append(test_split)
        return train_sets, test_sets
            
    def oversample(self, all_class_sets, max_sample_per_class):
        for i in range(len(all_class_sets)):
            num_to_duplicate = max_sample_per_class - all_class_sets[i][0].shape[0]
            if num_to_duplicate > 0:
                indexes_to_duplicate = np.random.choice(np.arange(all_class_sets[i][0].shape[0]), size=num_to_duplicate)
                for j in range(len(all_class_sets[i])):
                    all_class_sets[i][j] = torch.cat([all_class_sets[i][j], all_class_sets[i][j][indexes_to_duplicate]])
        return all_class_sets
    
    def combine_classes(self, all_class_sets, controlled=False):
        dataset = [[] for i in range(len(all_class_sets[0]))]
        
        for i, class_set in enumerate(all_class_sets):
            if controlled and self.class_sizes is not None:
                class_size = self.class_sizes[i] 
                indexes = np.random.choice(np.arange(class_set[0].shape[0]), size=class_size)
            else:
                indexes = np.arange(class_set[0].shape[0])

            for j, each in enumerate(class_set):
                dataset[j].append(each[indexes])
        
        for i in range(len(dataset)):
            dataset[i] = torch.cat(dataset[i])

        return dataset

    def load_dataset(self, test_split_ratio=0.2):
        datasets = self.create_dataset() ## [x, y]
        classes = list(datasets[-1].unique())
        
        ## for each class seprate [x,y] in all_class_sets
        all_class_sets = [] 
        max_sample_per_class = 0
        for each_class in classes:
            class_sets = []
            for each_dataset in datasets:
                class_samples = each_dataset[datasets[-1] == each_class]
                class_sets.append(class_samples)
            all_class_sets.append(class_sets)
            
            if class_sets[0].shape[0] > max_sample_per_class:
                max_sample_per_class = class_sets[0].shape[0]

        train_sets, test_sets = self.split_class_sets(all_class_sets, test_split_ratio)
        
        if self.load_method == "OVERSAMPLE_NON_MAX_TRAIN":
            max_sample_per_class = int(max_sample_per_class * (1-test_split_ratio))
            train_sets = self.oversample(train_sets[:], max_sample_per_class)

        dataset_train = shuffle(*self.combine_classes(train_sets, True), random_state=0)
        dataset_test = shuffle(*self.combine_classes(test_sets), random_state=0)

        print("Train:", dataset_train[0].shape[0])
        print("Test:", dataset_test[0].shape[0])
        print("Max sample per class:", max_sample_per_class)

        return dataset_train, dataset_test

import torch
import torchvision.datasets as datasets
from torchvision.transforms import transforms 

class MNISTLoader(BaseLoader):

    def __init__(self, cfg, checkpoint_folder=None):
        super(MNISTLoader, self).__init__(cfg)

        self.train_data = datasets.MNIST(root='raw_data_sets/', train=True, transform=transforms.ToTensor(), download=True)
        self.test_data = datasets.MNIST(root='raw_data_sets/', train=False, transform=transforms.ToTensor(), download=True)


    def create_dataset(self):
        
        x_train, y_train = self.train_data.data, self.train_data.targets
        x_test, y_test = self.test_data.data, self.test_data.targets

        x_train = x_train.view(x_train.shape[0], -1).type(torch.FloatTensor)
        x_test = x_test.view(x_test.shape[0], -1).type(torch.FloatTensor)

        return torch.cat([x_train, x_test]).to(self.device), torch.cat([y_train, y_test]).type(torch.LongTensor).to(self.device)
    
    def load_dataset(self, test_split_ratio=0.2):
        train, test = super().load_dataset(test_split_ratio)
        return train[0], train[1], test[0], test[1]

import datetime
from tabnanny import check

import torch
import torch.nn as nn
import torch.optim as optim
from pathlib import Path

import matplotlib.pyplot as plt
import shutil

class BaseModel:

    def __init__(self, cfg, checkpoint_folder, dataprocessor):

        self.checkpoint_folder = checkpoint_folder

        self.batch_size = cfg.BATCH_SIZE
        
        try:
            self.drop_prob = cfg.DROP_PROB
        except AttributeError:
            self.drop_prob = None

        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

        self.lr = cfg.LR
        self.total_epoch = cfg.EPOCH
        self.loss_fn_name = cfg.LOSS
        self.optim_name = cfg.OPTIMIZER
        self.PRINT_EVERY = cfg.PRINT_EVERY
        self.dataprocessor = dataprocessor
        
    def set_model(self, **kwargs):
        raise Exception("Not Implemented")

    def update_per_iter(self):
        pass
    
    def update_per_epoch(self):
        pass

    def train(self, train_loader):
        
        self.loss_fn = getattr(nn, self.loss_fn_name)()
        self.optimizer = getattr(optim, self.optim_name)(self.model.parameters(), lr=self.lr)

        ep = 1
        print_track_step = 0
        self.model.train()
        all_losses = []
        epoch_losses = []

        for epoch in range(self.total_epoch):
            batch_losses = []
            for batch_idx, example, in enumerate(train_loader):
                
                result = example[-1]
                scores = self.model(example[:-1]).squeeze(1)
                loss = self.loss_fn(scores, result)
                all_losses.append(loss.item())
                batch_losses.append(loss.item())
                self.optimizer.zero_grad()
                loss.backward()

                self.update_per_iter()
                
                self.optimizer.step()
                print_track_step += 1
                if print_track_step % self.PRINT_EVERY == 0:
                    print(f'epoch: {epoch + 1} step: {batch_idx + 1}/{len(train_loader)} loss: {loss}')
                    print_track_step = 0

            print(f'epoch: {epoch + 1} loss: {sum(batch_losses)/len(batch_losses)}')
            print_track_step = 0
            checkpoint_name = self.checkpoint_folder / Path("checkpoint_e"+str(epoch)+".pt")
            torch.save({
                        'epoch': epoch,
                        'model_state_dict': self.model.state_dict(),
                        'optimizer_state_dict': self.optimizer.state_dict(),
                        }, checkpoint_name)
            ep += 1
            epoch_losses.append(sum(batch_losses)/len(batch_losses))
            
            self.update_per_epoch()

        checkpoint_name = self.checkpoint_folder / Path("model.pt")
        torch.save({
                    'model_state_dict': self.model.state_dict(),
                    'optimizer_state_dict': self.optimizer.state_dict(),
                    'all_losses': all_losses,
                    'epoch_losses': epoch_losses,
                    }, checkpoint_name)

        plt.figure(0)          
        plt.plot([i for i in range(len(all_losses))], all_losses)
        plt.savefig(self.checkpoint_folder / Path("all_losses.png"))
        
        plt.figure(1)
        plt.plot([i for i in range(len(epoch_losses))], epoch_losses)
        plt.savefig(self.checkpoint_folder / Path("epoch_losses.png"))
        


    def predict(self, X, threshold=0.5):
        pass

    def fit(self, X, y_train):
        pass

import torch
import torch.nn as nn


class GradBasedDropout(nn.Module):
    def __init__(self, input_dim, drop_prob):
        super(GradBasedDropout, self).__init__()
        self.keep_prob = torch.ones(input_dim).to(device)
        self.drop_prob = drop_prob
    
    def update_keep_prob(self, grad, method):
        if method == "TANH":
            self.keep_prob = torch.tanh(torch.abs(grad).sum(dim=-1))
            if self.drop_prob is not None:
                # scale to 1-DROP_PROB to 1 if specified, else left as 0 to 1  range
                self.keep_prob = (self.keep_prob * self.drop_prob) + (1-self.drop_prob)
        elif method == "ABS_NORM":
            grad = torch.abs(grad).sum(dim=-1)
            self.keep_prob = (grad - torch.min(grad))/(torch.max(grad) - torch.min(grad) + 1e-7)
            if self.drop_prob is not None:
                # scale to 1-DROP_PROB to 1 if specified, else left as 0 to 1  range
                self.keep_prob = (self.keep_prob * self.drop_prob) + (1-self.drop_prob)
        elif method == "NORM":
            grad = grad.sum(dim=-1)
            self.keep_prob = (grad - torch.min(grad))/(torch.max(grad) - torch.min(grad) + 1e-7)
            if self.drop_prob is not None:
                # scale to 1-DROP_PROB to 1 if specified, else left as 0 to 1  range
                self.keep_prob = (self.keep_prob * self.drop_prob) + (1-self.drop_prob)
        
        ## The idea is to keep neurons with higher gradients stay
        ## and drop neurons with low gradients
        # print(self.drop_prob)
        # print(grad.max(), grad.min(), grad.mean())
        # print(self.keep_prob.max(), self.keep_prob.min(), self.keep_prob.mean())
        ## doing the opposite could be:
        # self.keep_prob = 1-self.keep_prob

    def forward(self, x):
        keep_prob = torch.ones(x.shape).to(device) * torch.unsqueeze(self.keep_prob, dim=0) # for all batches same probability
        keep_prob = torch.clip(keep_prob, min=0.00001, max=1)
        mask = torch.bernoulli(keep_prob) * 1/keep_prob
        return mask * x

class LinearWithGradBasedDropout(nn.Module):

    def __init__(self, input_dim, hidden_dim, dropout_prob):
        super(LinearWithGradBasedDropout, self).__init__()

        self.fc = nn.Linear(input_dim, hidden_dim)
        self.drop = GradBasedDropout(hidden_dim, dropout_prob)
        self.activation = nn.ReLU()

    def forward(self, x):
        return self.drop(self.activation(self.fc(x)))

class LinearWithNoDropout(nn.Module):

    def __init__(self, input_dim, hidden_dim):
        super(LinearWithNoDropout, self).__init__()
        self.fc = nn.Linear(input_dim, hidden_dim)
        self.activation = nn.ReLU()
    
    def forward(self, x):
        return self.activation(self.fc(x))

class LinearModel(nn.Module):

    def __init__(self, linear_layer, hidden_dim=2048, target_size=1):

        super(LinearModel, self).__init__()
        
        self.target_size = target_size
        
        self.linear_layer = linear_layer
        self.dense = nn.Linear(hidden_dim, target_size)
        if target_size > 1:
            self.out = nn.LogSoftmax(dim=-1)
        else:
            self.out = nn.Sigmoid()
        
    def forward(self, x):
        x = x[0]
        x = self.linear_layer(x)
        return self.out(self.dense(x))

class LinearModelNoDropout(BaseModel):

    def __init__(self, cfg, train_path, input_dim, dataprocessor, hidden_dim=2048):
        super(LinearModelNoDropout, self).__init__(cfg, train_path, dataprocessor)
        
        target_size = 1
        if cfg.OUTPUT_TYPE == "GRADE":
            target_size = 5
        elif cfg.OUTPUT_TYPE == "NUMBERS":
            target_size = 10
        self.set_model(input_dim, hidden_dim, target_size)

    def set_model(self, input_dim, hidden_dim, target_size):
        layer = LinearWithNoDropout(input_dim, hidden_dim).to(self.device)
        self.model = LinearModel(layer, hidden_dim, target_size).to(self.device)

    def predict(self, X, threshold=0.5):
        y_preds = []

        with torch.no_grad():
            self.model.eval()    
            for i in range(0,X.shape[0], self.batch_size):
                y_preds.append(self.model((X[i:min(X.shape[0], i+self.batch_size)],)))

        if y_preds[0].shape[-1] > 1:
            yp = torch.argmax(torch.cat(y_preds, dim=0), dim=1).cpu()
            return yp

        return torch.squeeze(torch.cat(y_preds,dim=0).cpu() > threshold, 1)

    def fit(self, X, y_train):
        train_dataset = torch.utils.data.TensorDataset(X, y_train)
        train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=self.batch_size)

        self.train(train_loader)

from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, precision_score, recall_score, roc_auc_score
from imblearn.metrics import geometric_mean_score as gmean_score

def confusion_matrix_dict(y_predicted, y_target):
    cm = confusion_matrix(y_target, y_predicted)
    return f"Confusion matrix = {cm}\n"
    

def get_f1_score(y_predicted, y_target):
    return f"F1 Score = {f1_score(y_target, y_predicted, average='macro')}\n"
    
def get_precision(y_predicted, y_target):
    return f"Precision = {precision_score(y_target, y_predicted, average='macro')}\n"

def get_recall(y_predicted, y_target): ### TODO: UAR consideration
    return f"Recall = {recall_score(y_target, y_predicted, average='macro')}\n"

def get_roc_auc(y_predicted, y_target):
    return f"ROC AUC = {roc_auc_score(y_target, y_predicted)}\n"

def get_gmean(y_predicted, y_target):
    return f"Geometric Mean = {gmean_score(y_target, y_predicted)}\n"

def get_accuracy(y_predicted, y_target):
    return f"Accuracy = {accuracy_score(y_target, y_predicted)}\n"

def get_evaluation_methods(cfg):
    evaluation_methods = cfg.EVALUATION_METHODS
    methods = []
    for each in evaluation_methods:
        if each == "CONFUSION_MATRIX":
            methods.append(confusion_matrix_dict)
        elif each == "ACCURACY":
            methods.append(get_accuracy)
        elif each == "F1_SCORE":
            methods.append(get_f1_score)
        elif each == "PRECISION":
            methods.append(get_precision)
        elif each == "RECALL" or each == "UAR":
            methods.append(get_recall)
        elif each == "GMEAN":
            methods.append(get_gmean)
        else:
            raise Exception(f"Requested evaluation method {each} is not supported")
    return methods 

def evaluate(cfg, model, X_train, X_test, y_train, y_test):
    evaluation_methods = get_evaluation_methods(cfg)
    evaluation_result = "Train\n"
    for each_method in evaluation_methods:
        y_preds = model.predict(X_train)
        evaluation_result += each_method(y_preds, y_train.cpu())
    evaluation_result += "\n\nTest\n"
    for each_method in evaluation_methods:
        y_preds = model.predict(X_test)
        evaluation_result += each_method(y_preds, y_test.cpu())
    print(evaluation_result)

# Seed 
torch.manual_seed(123)
torch.cuda.manual_seed(123)
np.random.seed(123)

"""## No Dropout"""

cfg = {
    'OUTPUT_TYPE': 'NUMBERS', 
    'PROB_METHOD': 'NORM',
    'DROP_PROB': 0.2,
    'LOSS': 'NLLLoss',
    'OPTIMIZER': 'SGD',
    'PRINT_EVERY': 1000,
    'LR': 0.001,
    'EPOCH': 10,
    'LOAD_METHOD': 'ORIGINAL',
    'BATCH_SIZE': 32,
    'EVALUATION_METHODS': ['ACCURACY', 'F1_SCORE', 'PRECISION', 'RECALL', 'GMEAN'],
}
from types import SimpleNamespace
cfg = SimpleNamespace(**cfg)
dataprocessor = MNISTLoader(cfg, None)
x_train, y_train, x_test, y_test = dataprocessor.load_dataset()
model = LinearModelNoDropout(cfg, "", 28*28, dataprocessor)
model.fit(x_train, y_train)
evaluate(cfg, model, x_train, x_test, y_train, y_test)

s = torch.tensor([-1,3,4,5])
g = torch.tensor([-3,0,2,-10])
m = torch.max(torch.abs(s), torch.abs(g))
print(m)
print(torch.sign(s))
print(m == torch.abs(s))
print( ((m == torch.abs(s)) * torch.sign(s)).int())
m = m * (((m == torch.abs(s)) * torch.sign(s)) |  ((m == torch.abs(g)) * torch.sign(g)))
print(m)

class LinearModelGradBasedDrop(LinearModelNoDropout):
    
    def __init__(self, cfg, train_path, input_dim, dataprocessor, **kwargs):
        super(LinearModelGradBasedDrop, self).__init__(cfg, train_path, input_dim, dataprocessor, **kwargs)
        self.prob_method = cfg.PROB_METHOD
    
    def set_model(self, input_dim, hidden_dim, target_size):
        layer = LinearWithGradBasedDropout(input_dim, hidden_dim, self.drop_prob)
        self.model = LinearModel(layer, hidden_dim, target_size).to(self.device)

    def update_per_iter(self):
        self.model.linear_layer.drop.update_keep_prob(self.model.linear_layer.fc.weight.grad.detach(), self.prob_method)

class LinearModelGradBasedDropV2(LinearModelNoDropout):
    
    def __init__(self, cfg, train_path, input_dim, dataprocessor, **kwargs):
        super(LinearModelGradBasedDropV2, self).__init__(cfg, train_path, input_dim, dataprocessor, **kwargs)
        self.prob_method = cfg.PROB_METHOD

        self.input_dim = input_dim
        self.grads = torch.zeros(2048, input_dim).to(device)
    
    def set_model(self, input_dim, hidden_dim, target_size):
        layer = LinearWithGradBasedDropout(input_dim, hidden_dim, self.drop_prob)
        self.model = LinearModel(layer, hidden_dim, target_size).to(self.device)

    def update_per_iter(self):
        self.grads += self.model.linear_layer.fc.weight.grad.detach()

    def update_per_epoch(self):
        self.model.linear_layer.drop.update_keep_prob(self.grads, self.prob_method)
        self.grads = torch.zeros(2048, self.input_dim).to(device)

class LinearModelGradBasedDropV3(LinearModelNoDropout):
    
    def __init__(self, cfg, train_path, input_dim, dataprocessor, **kwargs):
        super(LinearModelGradBasedDropV3, self).__init__(cfg, train_path, input_dim, dataprocessor, **kwargs)
        self.prob_method = cfg.PROB_METHOD
        self.grads = None
    
    def set_model(self, input_dim, hidden_dim, target_size):
        layer = LinearWithGradBasedDropout(input_dim, hidden_dim, self.drop_prob)
        self.model = LinearModel(layer, hidden_dim, target_size).to(self.device)

    def update_per_iter(self):
        if self.grads is None:
            self.grads = self.model.linear_layer.fc.weight.grad.detach()
        else:
            cur_grads = self.model.linear_layer.fc.weight.grad.detach()
            new_grads = torch.max(torch.abs(self.grads), torch.abs(cur_grads))
            new_grads = new_grads * (((new_grads == torch.abs(self.grads)) * torch.sign(self.grads)).int() | ((new_grads == torch.abs(cur_grads)) * torch.sign(cur_grads)).int())
            
            self.grads = new_grads


    def update_per_epoch(self):
        self.model.linear_layer.drop.update_keep_prob(self.grads, self.prob_method)
        self.grards = None

"""## V1"""

cfg = {
    'OUTPUT_TYPE': 'NUMBERS', 
    'PROB_METHOD': 'NORM',
    'DROP_PROB': 0.2,
    'LOSS': 'NLLLoss',
    'OPTIMIZER': 'SGD',
    'PRINT_EVERY': 1000,
    'LR': 0.001,
    'EPOCH': 10,
    'LOAD_METHOD': 'ORIGINAL',
    'BATCH_SIZE': 32,
    'EVALUATION_METHODS': ['ACCURACY', 'F1_SCORE', 'PRECISION', 'RECALL', 'GMEAN'],
}
from types import SimpleNamespace
cfg = SimpleNamespace(**cfg)
dataprocessor = MNISTLoader(cfg, None)
x_train, y_train, x_test, y_test = dataprocessor.load_dataset()
model = LinearModelGradBasedDrop(cfg, "", 28*28, dataprocessor)
model.fit(x_train, y_train)
evaluate(cfg, model, x_train, x_test, y_train, y_test)

cfg = {
    'OUTPUT_TYPE': 'NUMBERS', 
    'PROB_METHOD': 'NORM',
    'LOSS': 'NLLLoss',
    'OPTIMIZER': 'SGD',
    'PRINT_EVERY': 1000,
    'LR': 0.001,
    'EPOCH': 10,
    'LOAD_METHOD': 'ORIGINAL',
    'BATCH_SIZE': 32,
    'EVALUATION_METHODS': ['ACCURACY', 'F1_SCORE', 'PRECISION', 'RECALL', 'GMEAN'],
}
from types import SimpleNamespace
cfg = SimpleNamespace(**cfg)
dataprocessor = MNISTLoader(cfg, None)
x_train, y_train, x_test, y_test = dataprocessor.load_dataset()
model = LinearModelGradBasedDrop(cfg, "", 28*28, dataprocessor)
model.fit(x_train, y_train)
evaluate(cfg, model, x_train, x_test, y_train, y_test)

cfg = {
    'OUTPUT_TYPE': 'NUMBERS', 
    'PROB_METHOD': 'ABS_NORM',
    'DROP_PROB': 0.2,
    'LOSS': 'NLLLoss',
    'OPTIMIZER': 'SGD',
    'PRINT_EVERY': 1000,
    'LR': 0.001,
    'EPOCH': 10,
    'LOAD_METHOD': 'ORIGINAL',
    'BATCH_SIZE': 32,
    'EVALUATION_METHODS': ['ACCURACY', 'F1_SCORE', 'PRECISION', 'RECALL', 'GMEAN'],
}
from types import SimpleNamespace
cfg = SimpleNamespace(**cfg)
dataprocessor = MNISTLoader(cfg, None)
x_train, y_train, x_test, y_test = dataprocessor.load_dataset()
model = LinearModelGradBasedDrop(cfg, "", 28*28, dataprocessor)
model.fit(x_train, y_train)
evaluate(cfg, model, x_train, x_test, y_train, y_test)

cfg = {
    'OUTPUT_TYPE': 'NUMBERS', 
    'PROB_METHOD': 'ABS_NORM',
    'LOSS': 'NLLLoss',
    'OPTIMIZER': 'SGD',
    'PRINT_EVERY': 1000,
    'LR': 0.001,
    'EPOCH': 10,
    'LOAD_METHOD': 'ORIGINAL',
    'BATCH_SIZE': 32,
    'EVALUATION_METHODS': ['ACCURACY', 'F1_SCORE', 'PRECISION', 'RECALL', 'GMEAN'],
}
from types import SimpleNamespace
cfg = SimpleNamespace(**cfg)
dataprocessor = MNISTLoader(cfg, None)
x_train, y_train, x_test, y_test = dataprocessor.load_dataset()
model = LinearModelGradBasedDrop(cfg, "", 28*28, dataprocessor)
model.fit(x_train, y_train)
evaluate(cfg, model, x_train, x_test, y_train, y_test)

cfg = {
    'OUTPUT_TYPE': 'NUMBERS', 
    'PROB_METHOD': 'TANH',
    'LOSS': 'NLLLoss',
    'OPTIMIZER': 'SGD',
    'PRINT_EVERY': 1000,
    'LR': 0.001,
    'EPOCH': 10,
    'LOAD_METHOD': 'ORIGINAL',
    'BATCH_SIZE': 32,
    'EVALUATION_METHODS': ['ACCURACY', 'F1_SCORE', 'PRECISION', 'RECALL', 'GMEAN'],
}
from types import SimpleNamespace
cfg = SimpleNamespace(**cfg)
dataprocessor = MNISTLoader(cfg, None)
x_train, y_train, x_test, y_test = dataprocessor.load_dataset()
model = LinearModelGradBasedDrop(cfg, "", 28*28, dataprocessor)
model.fit(x_train, y_train)
evaluate(cfg, model, x_train, x_test, y_train, y_test)

cfg = {
    'OUTPUT_TYPE': 'NUMBERS', 
    'PROB_METHOD': 'TANH',
    'DROP_PROB': 0.2,
    'LOSS': 'NLLLoss',
    'OPTIMIZER': 'SGD',
    'PRINT_EVERY': 1000,
    'LR': 0.001,
    'EPOCH': 10,
    'LOAD_METHOD': 'ORIGINAL',
    'BATCH_SIZE': 32,
    'EVALUATION_METHODS': ['ACCURACY', 'F1_SCORE', 'PRECISION', 'RECALL', 'GMEAN'],
}
from types import SimpleNamespace
cfg = SimpleNamespace(**cfg)
dataprocessor = MNISTLoader(cfg, None)
x_train, y_train, x_test, y_test = dataprocessor.load_dataset()
model = LinearModelGradBasedDrop(cfg, "", 28*28, dataprocessor)
model.fit(x_train, y_train)
evaluate(cfg, model, x_train, x_test, y_train, y_test)

"""## V2"""

cfg = {
    'OUTPUT_TYPE': 'NUMBERS', 
    'PROB_METHOD': 'NORM',
    'DROP_PROB': 0.2,
    'LOSS': 'NLLLoss',
    'OPTIMIZER': 'SGD',
    'PRINT_EVERY': 1000,
    'LR': 0.001,
    'EPOCH': 10,
    'LOAD_METHOD': 'ORIGINAL',
    'BATCH_SIZE': 32,
    'EVALUATION_METHODS': ['ACCURACY', 'F1_SCORE', 'PRECISION', 'RECALL', 'GMEAN'],
}
from types import SimpleNamespace
cfg = SimpleNamespace(**cfg)
dataprocessor = MNISTLoader(cfg, None)
x_train, y_train, x_test, y_test = dataprocessor.load_dataset()
model = LinearModelGradBasedDropV2(cfg, "", 28*28, dataprocessor)
model.fit(x_train, y_train)
evaluate(cfg, model, x_train, x_test, y_train, y_test)

cfg = {
    'OUTPUT_TYPE': 'NUMBERS', 
    'PROB_METHOD': 'NORM',
    'LOSS': 'NLLLoss',
    'OPTIMIZER': 'SGD',
    'PRINT_EVERY': 1000,
    'LR': 0.001,
    'EPOCH': 10,
    'LOAD_METHOD': 'ORIGINAL',
    'BATCH_SIZE': 32,
    'EVALUATION_METHODS': ['ACCURACY', 'F1_SCORE', 'PRECISION', 'RECALL', 'GMEAN'],
}
from types import SimpleNamespace
cfg = SimpleNamespace(**cfg)
dataprocessor = MNISTLoader(cfg, None)
x_train, y_train, x_test, y_test = dataprocessor.load_dataset()
model = LinearModelGradBasedDropV2(cfg, "", 28*28, dataprocessor)
model.fit(x_train, y_train)
evaluate(cfg, model, x_train, x_test, y_train, y_test)

cfg = {
    'OUTPUT_TYPE': 'NUMBERS', 
    'PROB_METHOD': 'ABS_NORM',
    'DROP_PROB': 0.2,
    'LOSS': 'NLLLoss',
    'OPTIMIZER': 'SGD',
    'PRINT_EVERY': 1000,
    'LR': 0.001,
    'EPOCH': 10,
    'LOAD_METHOD': 'ORIGINAL',
    'BATCH_SIZE': 32,
    'EVALUATION_METHODS': ['ACCURACY', 'F1_SCORE', 'PRECISION', 'RECALL', 'GMEAN'],
}
from types import SimpleNamespace
cfg = SimpleNamespace(**cfg)
dataprocessor = MNISTLoader(cfg, None)
x_train, y_train, x_test, y_test = dataprocessor.load_dataset()
model = LinearModelGradBasedDropV2(cfg, "", 28*28, dataprocessor)
model.fit(x_train, y_train)
evaluate(cfg, model, x_train, x_test, y_train, y_test)

cfg = {
    'OUTPUT_TYPE': 'NUMBERS', 
    'PROB_METHOD': 'ABS_NORM',
    'LOSS': 'NLLLoss',
    'OPTIMIZER': 'SGD',
    'PRINT_EVERY': 1000,
    'LR': 0.001,
    'EPOCH': 10,
    'LOAD_METHOD': 'ORIGINAL',
    'BATCH_SIZE': 32,
    'EVALUATION_METHODS': ['ACCURACY', 'F1_SCORE', 'PRECISION', 'RECALL', 'GMEAN'],
}
from types import SimpleNamespace
cfg = SimpleNamespace(**cfg)
dataprocessor = MNISTLoader(cfg, None)
x_train, y_train, x_test, y_test = dataprocessor.load_dataset()
model = LinearModelGradBasedDropV2(cfg, "", 28*28, dataprocessor)
model.fit(x_train, y_train)
evaluate(cfg, model, x_train, x_test, y_train, y_test)

cfg = {
    'OUTPUT_TYPE': 'NUMBERS', 
    'PROB_METHOD': 'TANH',
    'LOSS': 'NLLLoss',
    'OPTIMIZER': 'SGD',
    'PRINT_EVERY': 1000,
    'LR': 0.001,
    'EPOCH': 10,
    'LOAD_METHOD': 'ORIGINAL',
    'BATCH_SIZE': 32,
    'EVALUATION_METHODS': ['ACCURACY', 'F1_SCORE', 'PRECISION', 'RECALL', 'GMEAN'],
}
from types import SimpleNamespace
cfg = SimpleNamespace(**cfg)
dataprocessor = MNISTLoader(cfg, None)
x_train, y_train, x_test, y_test = dataprocessor.load_dataset()
model = LinearModelGradBasedDropV2(cfg, "", 28*28, dataprocessor)
model.fit(x_train, y_train)
evaluate(cfg, model, x_train, x_test, y_train, y_test)

cfg = {
    'OUTPUT_TYPE': 'NUMBERS', 
    'PROB_METHOD': 'TANH',
    'DROP_PROB': 0.2,
    'LOSS': 'NLLLoss',
    'OPTIMIZER': 'SGD',
    'PRINT_EVERY': 1000,
    'LR': 0.001,
    'EPOCH': 10,
    'LOAD_METHOD': 'ORIGINAL',
    'BATCH_SIZE': 32,
    'EVALUATION_METHODS': ['ACCURACY', 'F1_SCORE', 'PRECISION', 'RECALL', 'GMEAN'],
}
from types import SimpleNamespace
cfg = SimpleNamespace(**cfg)
dataprocessor = MNISTLoader(cfg, None)
x_train, y_train, x_test, y_test = dataprocessor.load_dataset()
model = LinearModelGradBasedDropV2(cfg, "", 28*28, dataprocessor)
model.fit(x_train, y_train)
evaluate(cfg, model, x_train, x_test, y_train, y_test)

"""# V3"""

cfg = {
    'OUTPUT_TYPE': 'NUMBERS', 
    'PROB_METHOD': 'NORM',
    'DROP_PROB': 0.2,
    'LOSS': 'NLLLoss',
    'OPTIMIZER': 'SGD',
    'PRINT_EVERY': 1000,
    'LR': 0.001,
    'EPOCH': 10,
    'LOAD_METHOD': 'ORIGINAL',
    'BATCH_SIZE': 32,
    'EVALUATION_METHODS': ['ACCURACY', 'F1_SCORE', 'PRECISION', 'RECALL', 'GMEAN'],
}
from types import SimpleNamespace
cfg = SimpleNamespace(**cfg)
dataprocessor = MNISTLoader(cfg, None)
x_train, y_train, x_test, y_test = dataprocessor.load_dataset()
model = LinearModelGradBasedDropV3(cfg, "", 28*28, dataprocessor)
model.fit(x_train, y_train)
evaluate(cfg, model, x_train, x_test, y_train, y_test)

cfg = {
    'OUTPUT_TYPE': 'NUMBERS', 
    'PROB_METHOD': 'NORM',
    'LOSS': 'NLLLoss',
    'OPTIMIZER': 'SGD',
    'PRINT_EVERY': 1000,
    'LR': 0.001,
    'EPOCH': 10,
    'LOAD_METHOD': 'ORIGINAL',
    'BATCH_SIZE': 32,
    'EVALUATION_METHODS': ['ACCURACY', 'F1_SCORE', 'PRECISION', 'RECALL', 'GMEAN'],
}
from types import SimpleNamespace
cfg = SimpleNamespace(**cfg)
dataprocessor = MNISTLoader(cfg, None)
x_train, y_train, x_test, y_test = dataprocessor.load_dataset()
model = LinearModelGradBasedDropV3(cfg, "", 28*28, dataprocessor)
model.fit(x_train, y_train)
evaluate(cfg, model, x_train, x_test, y_train, y_test)

cfg = {
    'OUTPUT_TYPE': 'NUMBERS', 
    'PROB_METHOD': 'ABS_NORM',
    'DROP_PROB': 0.2,
    'LOSS': 'NLLLoss',
    'OPTIMIZER': 'SGD',
    'PRINT_EVERY': 1000,
    'LR': 0.001,
    'EPOCH': 10,
    'LOAD_METHOD': 'ORIGINAL',
    'BATCH_SIZE': 32,
    'EVALUATION_METHODS': ['ACCURACY', 'F1_SCORE', 'PRECISION', 'RECALL', 'GMEAN'],
}
from types import SimpleNamespace
cfg = SimpleNamespace(**cfg)
dataprocessor = MNISTLoader(cfg, None)
x_train, y_train, x_test, y_test = dataprocessor.load_dataset()
model = LinearModelGradBasedDropV3(cfg, "", 28*28, dataprocessor)
model.fit(x_train, y_train)
evaluate(cfg, model, x_train, x_test, y_train, y_test)

cfg = {
    'OUTPUT_TYPE': 'NUMBERS', 
    'PROB_METHOD': 'ABS_NORM',
    'LOSS': 'NLLLoss',
    'OPTIMIZER': 'SGD',
    'PRINT_EVERY': 1000,
    'LR': 0.001,
    'EPOCH': 10,
    'LOAD_METHOD': 'ORIGINAL',
    'BATCH_SIZE': 32,
    'EVALUATION_METHODS': ['ACCURACY', 'F1_SCORE', 'PRECISION', 'RECALL', 'GMEAN'],
}
from types import SimpleNamespace
cfg = SimpleNamespace(**cfg)
dataprocessor = MNISTLoader(cfg, None)
x_train, y_train, x_test, y_test = dataprocessor.load_dataset()
model = LinearModelGradBasedDropV3(cfg, "", 28*28, dataprocessor)
model.fit(x_train, y_train)
evaluate(cfg, model, x_train, x_test, y_train, y_test)

cfg = {
    'OUTPUT_TYPE': 'NUMBERS', 
    'PROB_METHOD': 'TANH',
    'LOSS': 'NLLLoss',
    'OPTIMIZER': 'SGD',
    'PRINT_EVERY': 1000,
    'LR': 0.001,
    'EPOCH': 10,
    'LOAD_METHOD': 'ORIGINAL',
    'BATCH_SIZE': 32,
    'EVALUATION_METHODS': ['ACCURACY', 'F1_SCORE', 'PRECISION', 'RECALL', 'GMEAN'],
}
from types import SimpleNamespace
cfg = SimpleNamespace(**cfg)
dataprocessor = MNISTLoader(cfg, None)
x_train, y_train, x_test, y_test = dataprocessor.load_dataset()
model = LinearModelGradBasedDropV3(cfg, "", 28*28, dataprocessor)
model.fit(x_train, y_train)
evaluate(cfg, model, x_train, x_test, y_train, y_test)

cfg = {
    'OUTPUT_TYPE': 'NUMBERS', 
    'PROB_METHOD': 'TANH',
    'DROP_PROB': 0.2,
    'LOSS': 'NLLLoss',
    'OPTIMIZER': 'SGD',
    'PRINT_EVERY': 1000,
    'LR': 0.001,
    'EPOCH': 10,
    'LOAD_METHOD': 'ORIGINAL',
    'BATCH_SIZE': 32,
    'EVALUATION_METHODS': ['ACCURACY', 'F1_SCORE', 'PRECISION', 'RECALL', 'GMEAN'],
}
from types import SimpleNamespace
cfg = SimpleNamespace(**cfg)
dataprocessor = MNISTLoader(cfg, None)
x_train, y_train, x_test, y_test = dataprocessor.load_dataset()
model = LinearModelGradBasedDropV3(cfg, "", 28*28, dataprocessor)
model.fit(x_train, y_train)
evaluate(cfg, model, x_train, x_test, y_train, y_test)

